{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUehXgCyIRdq"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "*   Alumno 1: Pablo Cegarra\n",
        "*   Alumno 2: David Gonzalez\n",
        "*   Alumno 3: Astrid Gamoneda\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU2BPrK2JkP0"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_YDFwZ-JscI"
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6n7MIefJ21i",
        "outputId": "d41105a2-3567-4a9a-d05c-cfad560ae2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "/content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "Archivos en el directorio: \n",
            "['dqn_SpaceInvaders-v0_weights_25000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_25000.h5f.index', 'dqn_SpaceInvaders-v0_weights_50000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_50000.h5f.index', 'dqn_SpaceInvaders-v0_weights_75000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_75000.h5f.index', 'dqn_SpaceInvaders-v0_weights_100000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_100000.h5f.index', 'dqn_SpaceInvaders-v0_weights_125000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_125000.h5f.index', 'dqn_SpaceInvaders-v0_weights_250000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_250000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_450000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_450000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_475000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_475000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_525000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_525000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_550000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_550000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_575000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_575000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_600000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_600000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_625000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_625000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_650000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_650000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_25000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_25000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_50000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_50000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_75000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_75000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_100000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_100000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_125000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_125000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_150000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_150000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_175000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_175000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_200000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_200000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_225000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_225000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_250000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_250000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_275000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_275000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_log.json', 'dqn_SpaceInvaders-v0_weights_2_300000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_300000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_325000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_325000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_350000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_350000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_375000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_375000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_2_400000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_400000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_log_2.json', 'dqn_SpaceInvaders-v0_weights_2_425000.h5f.index', 'dqn_SpaceInvaders-v0_weights_2_425000.h5f.data-00000-of-00001', 'checkpoint']\n"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UbVRjvHCJ8UF",
        "outputId": "0f5c6e3f-dd59-4a04-a94c-3e0aadd8c780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.17.3\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.25.0)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654622 sha256=c376b481d1b179d63ab26c3031649a29c5b8910bb1c802b6c8cec8106e193a86\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, cloudpickle, gym\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-x9g6apwz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-x9g6apwz\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.25.0)\n",
            "Building wheels for collected packages: atari-py\n",
            "  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4812072 sha256=a86c1b9e056d2c258d484b796912f4293bae5db45f8bc0561176a35b8089e386\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qd17dklu/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n",
            "Successfully built atari-py\n",
            "Installing collected packages: atari-py\n",
            "Successfully installed atari-py-1.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2==1.0.5\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.3.25)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.0)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
            "Installing collected packages: numpy, keras-rl2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.0\n",
            "    Uninstalling numpy-1.25.0:\n",
            "      Successfully uninstalled numpy-1.25.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\n",
            "flax 0.6.9 requires jax>=0.4.2, but you have jax 0.3.25 which is incompatible.\n",
            "orbax-checkpoint 0.2.1 requires jax>=0.4.8, but you have jax 0.3.25 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-rl2-1.0.5 numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.3.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.8.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.32.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.54.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n",
            "Installing collected packages: tf-estimator-nightly, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.2\n",
            "    Uninstalling tensorboard-2.12.2:\n",
            "      Successfully uninstalled tensorboard-2.12.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.8\n",
        "else:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, una solución óptima será alcanzada cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3eRhgI-Gb2a"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "#### Configuración base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jGEZUcpGb2a"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (84, 84)  # Tamaño de la imagen de entrada\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwOE6I_KGb2a"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = 'SpaceInvaders-v0'  # Nombre del entorno\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "\n",
        "####**1. Implementación de la red neuronal**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4GKrfWSGb2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ede37d-d3f1-443a-b895-c65286aae6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " permute (Permute)           (None, 84, 84, 4)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 20, 20, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 9, 9, 64)          0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1606144   \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,687,206\n",
            "Trainable params: 1,687,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "input_shape = (4,) + INPUT_SHAPE  # Tamaño de entrada de la red neuronal\n",
        "\n",
        "# Definir la arquitectura del modelo\n",
        "model = Sequential()\n",
        "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9-_5HPGb2b"
      },
      "source": [
        "####**2. Implementación de la solución DQN**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyeDUAPLWqF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea7f7a6-5524-425f-9abc-1abae52b0c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Configurar el procesador de Atari\n",
        "processor = AtariProcessor()\n",
        "\n",
        "# Crear la memoria para almacenar la experiencia\n",
        "memory = SequentialMemory(limit=1000000, window_length=4)\n",
        "\n",
        "# Definir la política de exploración\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)\n",
        "\n",
        "# Crear el agente DQN\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory, processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000, train_interval=4, delta_clip=1.)\n",
        "\n",
        "# Compilar el agente DQN\n",
        "dqn.compile(Adam(lr=0.00025), metrics=['mae'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**2.1. Entrenamiento**"
      ],
      "metadata": {
        "id": "sCKaGgSF_qtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hlp3ZrD-W1aL"
      },
      "outputs": [],
      "source": [
        "weights_filename = 'dqn_{}_weights_2.h5f'.format(ENV_NAME)\n",
        "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_2_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log_2.json'.format(ENV_NAME)\n",
        "\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=25000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "\n",
        "dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
        "\n",
        "# Guarda los pesos del modelo\n",
        "dqn.save_weights(weights_filename, overwrite=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**2.2. Reentrenamiento desde el último checkpoint**"
      ],
      "metadata": {
        "id": "jqoebXFS_j7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU3PNtNduY3Y",
        "outputId": "47b6e44b-cdac-4bad-8d27-71c9a2207a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 1750000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 46s 5ms/step - reward: 0.0141\n",
            "14 episodes - episode_reward: 9.714 [5.000, 23.000] - ale.lives: 2.140\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 46s 5ms/step - reward: 0.0141\n",
            "13 episodes - episode_reward: 10.923 [5.000, 25.000] - ale.lives: 2.123\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 47s 5ms/step - reward: 0.0139\n",
            "15 episodes - episode_reward: 9.333 [4.000, 23.000] - ale.lives: 2.019\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 47s 5ms/step - reward: 0.0132\n",
            "15 episodes - episode_reward: 8.600 [4.000, 27.000] - ale.lives: 2.031\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 47s 5ms/step - reward: 0.0126\n",
            "15 episodes - episode_reward: 8.400 [4.000, 20.000] - ale.lives: 2.134\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 306s 31ms/step - reward: 0.0137\n",
            "15 episodes - episode_reward: 9.467 [3.000, 20.000] - loss: 0.016 - mae: 1.874 - mean_q: 2.275 - mean_eps: 0.951 - ale.lives: 2.053\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 306s 31ms/step - reward: 0.0138\n",
            "15 episodes - episode_reward: 8.467 [2.000, 17.000] - loss: 0.015 - mae: 1.914 - mean_q: 2.327 - mean_eps: 0.942 - ale.lives: 2.239\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 307s 31ms/step - reward: 0.0147\n",
            "15 episodes - episode_reward: 9.800 [4.000, 17.000] - loss: 0.014 - mae: 1.945 - mean_q: 2.366 - mean_eps: 0.933 - ale.lives: 2.139\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0167\n",
            "14 episodes - episode_reward: 12.071 [3.000, 23.000] - loss: 0.014 - mae: 1.963 - mean_q: 2.388 - mean_eps: 0.924 - ale.lives: 2.152\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0158\n",
            "14 episodes - episode_reward: 12.000 [4.000, 19.000] - loss: 0.014 - mae: 2.016 - mean_q: 2.455 - mean_eps: 0.915 - ale.lives: 1.950\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 314s 31ms/step - reward: 0.0174\n",
            "16 episodes - episode_reward: 10.875 [4.000, 21.000] - loss: 0.015 - mae: 2.067 - mean_q: 2.518 - mean_eps: 0.906 - ale.lives: 2.051\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 316s 32ms/step - reward: 0.0157\n",
            "16 episodes - episode_reward: 9.500 [4.000, 20.000] - loss: 0.015 - mae: 2.105 - mean_q: 2.563 - mean_eps: 0.897 - ale.lives: 2.132\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0176\n",
            "14 episodes - episode_reward: 12.714 [4.000, 31.000] - loss: 0.015 - mae: 2.133 - mean_q: 2.598 - mean_eps: 0.888 - ale.lives: 2.018\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 314s 31ms/step - reward: 0.0166\n",
            "15 episodes - episode_reward: 11.200 [6.000, 20.000] - loss: 0.015 - mae: 2.149 - mean_q: 2.616 - mean_eps: 0.879 - ale.lives: 2.051\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 316s 32ms/step - reward: 0.0163\n",
            "14 episodes - episode_reward: 11.214 [2.000, 19.000] - loss: 0.015 - mae: 2.187 - mean_q: 2.663 - mean_eps: 0.870 - ale.lives: 2.083\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 318s 32ms/step - reward: 0.0176\n",
            "16 episodes - episode_reward: 11.438 [4.000, 29.000] - loss: 0.016 - mae: 2.214 - mean_q: 2.695 - mean_eps: 0.861 - ale.lives: 2.037\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0164\n",
            "15 episodes - episode_reward: 10.600 [4.000, 20.000] - loss: 0.016 - mae: 2.229 - mean_q: 2.713 - mean_eps: 0.852 - ale.lives: 2.233\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 322s 32ms/step - reward: 0.0166\n",
            "14 episodes - episode_reward: 11.500 [5.000, 21.000] - loss: 0.016 - mae: 2.299 - mean_q: 2.798 - mean_eps: 0.843 - ale.lives: 2.190\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0171\n",
            "16 episodes - episode_reward: 10.938 [4.000, 19.000] - loss: 0.016 - mae: 2.333 - mean_q: 2.840 - mean_eps: 0.834 - ale.lives: 2.107\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0163\n",
            "15 episodes - episode_reward: 10.400 [5.000, 19.000] - loss: 0.016 - mae: 2.362 - mean_q: 2.875 - mean_eps: 0.825 - ale.lives: 2.125\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0157\n",
            "15 episodes - episode_reward: 11.133 [4.000, 23.000] - loss: 0.016 - mae: 2.415 - mean_q: 2.938 - mean_eps: 0.816 - ale.lives: 2.145\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 326s 33ms/step - reward: 0.0178\n",
            "14 episodes - episode_reward: 12.071 [3.000, 20.000] - loss: 0.017 - mae: 2.452 - mean_q: 2.983 - mean_eps: 0.807 - ale.lives: 1.931\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 354s 35ms/step - reward: 0.0178\n",
            "16 episodes - episode_reward: 11.250 [4.000, 21.000] - loss: 0.017 - mae: 2.475 - mean_q: 3.011 - mean_eps: 0.798 - ale.lives: 1.956\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 365s 36ms/step - reward: 0.0188\n",
            "14 episodes - episode_reward: 13.571 [9.000, 20.000] - loss: 0.017 - mae: 2.502 - mean_q: 3.042 - mean_eps: 0.789 - ale.lives: 2.157\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 365s 36ms/step - reward: 0.0174\n",
            "15 episodes - episode_reward: 12.133 [4.000, 31.000] - loss: 0.018 - mae: 2.557 - mean_q: 3.109 - mean_eps: 0.780 - ale.lives: 2.076\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 372s 37ms/step - reward: 0.0168\n",
            "14 episodes - episode_reward: 11.929 [5.000, 26.000] - loss: 0.017 - mae: 2.568 - mean_q: 3.122 - mean_eps: 0.771 - ale.lives: 2.113\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 377s 38ms/step - reward: 0.0148\n",
            "15 episodes - episode_reward: 9.867 [4.000, 20.000] - loss: 0.018 - mae: 2.634 - mean_q: 3.201 - mean_eps: 0.762 - ale.lives: 1.972\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 366s 37ms/step - reward: 0.0188\n",
            "13 episodes - episode_reward: 14.154 [7.000, 25.000] - loss: 0.018 - mae: 2.634 - mean_q: 3.199 - mean_eps: 0.753 - ale.lives: 2.048\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 368s 37ms/step - reward: 0.0164\n",
            "12 episodes - episode_reward: 13.417 [8.000, 17.000] - loss: 0.019 - mae: 2.657 - mean_q: 3.225 - mean_eps: 0.744 - ale.lives: 2.064\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 375s 38ms/step - reward: 0.0158\n",
            "15 episodes - episode_reward: 11.000 [4.000, 17.000] - loss: 0.020 - mae: 2.656 - mean_q: 3.225 - mean_eps: 0.735 - ale.lives: 2.167\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 375s 38ms/step - reward: 0.0155\n",
            "16 episodes - episode_reward: 8.812 [2.000, 17.000] - loss: 0.020 - mae: 2.687 - mean_q: 3.264 - mean_eps: 0.726 - ale.lives: 2.125\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 374s 37ms/step - reward: 0.0172\n",
            "14 episodes - episode_reward: 12.929 [7.000, 18.000] - loss: 0.021 - mae: 2.728 - mean_q: 3.313 - mean_eps: 0.717 - ale.lives: 2.084\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 378s 38ms/step - reward: 0.0186\n",
            "13 episodes - episode_reward: 14.231 [4.000, 24.000] - loss: 0.020 - mae: 2.742 - mean_q: 3.331 - mean_eps: 0.708 - ale.lives: 1.966\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 380s 38ms/step - reward: 0.0182\n",
            "13 episodes - episode_reward: 11.846 [5.000, 30.000] - loss: 0.020 - mae: 2.801 - mean_q: 3.402 - mean_eps: 0.699 - ale.lives: 2.184\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 379s 38ms/step - reward: 0.0181\n",
            "16 episodes - episode_reward: 12.875 [7.000, 35.000] - loss: 0.021 - mae: 2.827 - mean_q: 3.431 - mean_eps: 0.690 - ale.lives: 2.087\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 382s 38ms/step - reward: 0.0169\n",
            "15 episodes - episode_reward: 11.600 [6.000, 25.000] - loss: 0.022 - mae: 2.839 - mean_q: 3.447 - mean_eps: 0.681 - ale.lives: 2.246\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 387s 39ms/step - reward: 0.0201\n",
            "15 episodes - episode_reward: 13.600 [8.000, 24.000] - loss: 0.021 - mae: 2.865 - mean_q: 3.477 - mean_eps: 0.672 - ale.lives: 2.075\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 385s 39ms/step - reward: 0.0182\n",
            "14 episodes - episode_reward: 13.071 [4.000, 28.000] - loss: 0.021 - mae: 2.900 - mean_q: 3.518 - mean_eps: 0.663 - ale.lives: 1.912\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 391s 39ms/step - reward: 0.0193\n",
            "14 episodes - episode_reward: 13.500 [5.000, 26.000] - loss: 0.022 - mae: 2.937 - mean_q: 3.563 - mean_eps: 0.654 - ale.lives: 1.955\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0155\n",
            "14 episodes - episode_reward: 10.500 [5.000, 16.000] - loss: 0.022 - mae: 2.962 - mean_q: 3.591 - mean_eps: 0.645 - ale.lives: 2.080\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 381s 38ms/step - reward: 0.0195\n",
            "15 episodes - episode_reward: 12.933 [5.000, 20.000] - loss: 0.021 - mae: 2.981 - mean_q: 3.614 - mean_eps: 0.636 - ale.lives: 2.040\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 373s 37ms/step - reward: 0.0169\n",
            "15 episodes - episode_reward: 12.000 [5.000, 21.000] - loss: 0.022 - mae: 3.031 - mean_q: 3.676 - mean_eps: 0.627 - ale.lives: 2.118\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 374s 37ms/step - reward: 0.0162\n",
            "17 episodes - episode_reward: 9.294 [3.000, 21.000] - loss: 0.022 - mae: 3.074 - mean_q: 3.725 - mean_eps: 0.618 - ale.lives: 2.111\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 375s 38ms/step - reward: 0.0176\n",
            "14 episodes - episode_reward: 12.000 [4.000, 21.000] - loss: 0.023 - mae: 3.068 - mean_q: 3.718 - mean_eps: 0.609 - ale.lives: 2.144\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0200"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-11-aabca20631f8>\", line 13, in <cell line: 13>\n",
            "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/core.py\", line 204, in fit\n",
            "    callbacks.on_step_end(episode_step, step_logs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/callbacks.py\", line 84, in on_step_end\n",
            "    callback.on_step_end(step, logs=logs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/callbacks.py\", line 381, in on_step_end\n",
            "    self.model.save_weights(filepath, overwrite=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/agents/dqn.py\", line 209, in save_weights\n",
            "    self.model.save_weights(filepath, overwrite=overwrite)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 39, in format_list\n",
            "    return StackSummary.from_list(extracted_list).format()\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 401, in from_list\n",
            "    filename, lineno, name, line = frame\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-11-aabca20631f8>\", line 13, in <cell line: 13>\n",
            "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/core.py\", line 204, in fit\n",
            "    callbacks.on_step_end(episode_step, step_logs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/callbacks.py\", line 84, in on_step_end\n",
            "    callback.on_step_end(step, logs=logs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/callbacks.py\", line 381, in on_step_end\n",
            "    self.model.save_weights(filepath, overwrite=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/agents/dqn.py\", line 209, in save_weights\n",
            "    self.model.save_weights(filepath, overwrite=overwrite)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 39, in format_list\n",
            "    return StackSummary.from_list(extracted_list).format()\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 401, in from_list\n",
            "    filename, lineno, name, line = frame\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-11-aabca20631f8>\", line 13, in <cell line: 13>\n",
            "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/core.py\", line 204, in fit\n",
            "    callbacks.on_step_end(episode_step, step_logs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/callbacks.py\", line 84, in on_step_end\n",
            "    callback.on_step_end(step, logs=logs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/callbacks.py\", line 381, in on_step_end\n",
            "    self.model.save_weights(filepath, overwrite=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/rl/agents/dqn.py\", line 209, in save_weights\n",
            "    self.model.save_weights(filepath, overwrite=overwrite)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 39, in format_list\n",
            "    return StackSummary.from_list(extracted_list).format()\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 401, in from_list\n",
            "    filename, lineno, name, line = frame\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "weights_filename = 'dqn_{}_weights_2.h5f'.format(ENV_NAME)\n",
        "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_2_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log_2.json'.format(ENV_NAME)\n",
        "\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=25000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "\n",
        "# Cargar los pesos del último punto de control\n",
        "dqn.load_weights('dqn_SpaceInvaders-v0_weights_2_650000.h5f')\n",
        "\n",
        "# Continuar el entrenamiento desde el último paso\n",
        "dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
        "\n",
        "# Guardar los pesos del modelo después del entrenamiento adicional\n",
        "dqn.save_weights(weights_filename, overwrite=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**2.3. Testing para obtener la media de recompensa**"
      ],
      "metadata": {
        "id": "rqtSz-X7_4Z6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHYryKd1Gb2b",
        "outputId": "e67577e5-e0ef-4afa-8382-9c8b03b751b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 12.000, steps: 763\n",
            "Episode 2: reward: 18.000, steps: 745\n",
            "Episode 3: reward: 16.000, steps: 678\n",
            "Episode 4: reward: 21.000, steps: 877\n",
            "Episode 5: reward: 16.000, steps: 711\n",
            "Episode 6: reward: 24.000, steps: 994\n",
            "Episode 7: reward: 32.000, steps: 1435\n",
            "Episode 8: reward: 16.000, steps: 848\n",
            "Episode 9: reward: 22.000, steps: 1148\n",
            "Episode 10: reward: 28.000, steps: 1037\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f21dc05c2e0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Testing\n",
        "dqn.load_weights('dqn_SpaceInvaders-v0_weights_2_425000.h5f')\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAlu8b1Gb2b"
      },
      "source": [
        "####**3. Justificación de los parámetros seleccionados y de los resultados obtenidos**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La selección de parámetros se ha llevado a cabo principalmente en base a la bibliografía encontrada al respecto y las clases prácticas de la asignatura de Aprendizaje por Refuerzo. Cabe destacar que el proceso de entrenamiento ha sido tedioso debido a la cantidad de recursos computacionales requeridos para poder ejecutar el algoritmo en un tiempo razonable. Esto ha impedido una mejor optimización de los parámetros ya que se ha priorizado alcanzar el requisito de minima puntuación para a partir de ahí plantear otros escenarios posibles.\n",
        "\n",
        "* Como procesador se ha utilizado el AtariProcessor, el cual ha sido utilizado a lo largo de la asignatura. Este procesador se utiliza para preprocesar las imágenes del entorno del juego Atari. Aplica una serie de transformaciones a las imágenes de entrada, como convertir a escala de grises, cambiar el tamaño y normalizar los valores de píxeles. Esto es necesario para que la red neuronal pueda procesar la información de manera adecuada.\n",
        "\n",
        "* Por un lado, el parámetro INPUT_SHAPE se ha fijado en 84 x 84 al ser un tamaño comunmente utilizado en otros algoritmos. Un menor tamaño podría haber agilizado el proceso de entrenamiento aunque a su vez la perdida de información podría ser relevante y empeorarlo. Por el contrario, un tamaño superior haría que el tiempo de cómputo aumentase y por tanto sería inviable de afrontar dados los recursos computacionales disponibles.\n",
        "\n",
        "* En cuanto al parámetro WINDOWS_SIZE, se ha intentado aumentar dicha ventana pensando que disponer de más información relativa a estados anteriores podría mejorar la sulución aumentando la recompensa esperada a futuro. Sin embargo no se ha conseguido dicho fin y se ha utilizado por defecto una ventana de 4 estados.\n",
        "\n",
        "* En lo relativo a la estructura de la red neuronal, se ha implementado una arquitectura típica partiendo de 3 capas convolucionales y 2 capas densas para obtener una salida de 6 neuronas correspondiente a cada una de las acciones que el agente puede realizar con una función de activación de salida lineal. Ugualmente se ha probado a introducir una capa adicional de 256 neuronas antes de la capa de slida sin obtener mejora aparente en los primeros 200000 steps del entrenamiento. Con lo cual se ha optado por dejar la configuración original.\n",
        "\n",
        "* En cuanto a la policy utilizada se ha optado por una de tipo epsilon-greedy para la exploración del agente. En nuestro caso la política específica utilizada es EpsGreedyQPolicy() dentro de la clase LinearAnnealedPolicy. Esta política hace que el agente tome decisiones basadas en la explotación de acciones con la mayor recompensa esperada. El parámetro epsilon controla el equilibrio entre la exploración y la explotación. Utilizamos valor máximo inicial de epsilon (exploración) de 1.0, un valor mínimo de epsilon de 0.1 y un valor de epsilon de prueba de 0.05. Además, la política es linealmente decreciente para reducir el valor de epsilon a lo largo de los primeros 1.000.000 de pasos.\n",
        "\n",
        "* Para el agente, tal y como se especifica en el enunciado, es necesario emplear Deep Q network. Para ello este se ha configurado con un modelo de red neuronal que hemos definido previamente para estimar los pesos idóneos, el número de acciones nb_actions que en nuestro juego son 6. Además, se especifica un número de pasos de calentamiento (nb_steps_warmup) de 50.000, un factor de descuento (gamma) de 0.99, un intervalo de actualización del modelo objetivo (target_model_update) de 10.000 pasos, un intervalo de entrenamiento (train_interval) de 4 pasos y un límite para el clip de error (delta_clip) de 1. Estos valores se seleccionan en función de experimentaciones y ajustes para obtener un buen rendimiento en el entrenamiento del agente.\n",
        "\n",
        "* Hemos definido una memoria secuencial para almacenar la experiencia del agente. En este caso, se utiliza una memoria con límite de un millón de muestras y una longitud de ventana de 4. Esto significa que el agente recordará las últimas 4 observaciones del entorno en cada paso, lo cual es útil para capturar información temporal en juegos Atari.\n",
        "\n",
        "* Finalmente, para la optimización de la red se ha empleado descenso de gradiente, concretamente el método de Adams utilizando una métrica de error absoluto medio para evaluar el rendimiento del modelo durante el entrenamiento y con una learning rate de 2.5e-4. Del mismo modo, al comprobar que el entrenamiento estaba siendo demasiado lento, se ha tratado de aumentar este valor sin conseguir mejoras aparentes e incluso observando un empeoramiento de los resultados.\n",
        "\n",
        "Como conclusión nos gustaría remarcar que este trabajo ha servido para comprobar la complejidad del proceso de entrenamiento de un algoritmo como DQN. Sin embargo, nos hubiese gustado poder dedicar más tiempo al diseño del mismo que al proceso de entrenamiento. Aún así, se ha alcanzado el requisito mínimo y el modelo funciona correctamente con los pesos que se adjuntan a este ejercicio."
      ],
      "metadata": {
        "id": "RTgrAi42jBm8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFQiicXK3sO"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}